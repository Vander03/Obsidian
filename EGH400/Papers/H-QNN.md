**Motivation**
- Implementing a hybrid QNN model that leverages quantum in the current NISQ era, where physical implementations are limited by noise

**Proposal**
- 2-qubit, six-layer H-QNN model designed to meet the qubit limitations of NISQ devices for image classification.
- Offers an efficient solution in terms of achieving high accuracy with **smaller number of images** for binary classification tasks when compared to CNN architectures

**Contributions/Methodology**
- Makes use of a parameterised quantum circuit as a hidden layer. This takes a classical input vector and configures the rotation angles of the quantum gates. The output of from the preceding neural network layer is fed into this parameterised circuit

![[Pasted image 20250820111607.png]]
- Algorithm 1 provides the simplified interface for creating and executing parameterised quantum circuits. A barrier is introduced (line 6) to demarcate different sections of the circuit. The barrier acts like a synchronization point in parallel computing, ensuring that operations before and after the barrier are treated as separate during optimization **(transpilation)**.
![[Pasted image 20250820130434.png]]

- Algorithm 2 provides forward and backward pass methods for gradient calculation
![[Pasted image 20250820131022.png]]

- Uses parameter-shift rule to calculate gradients in quantum circuits
- The parameter-shift rule allows the compution of the gradient of a parameterized quantum circuit, which is essential for optimizing quantum algorithms.  The rule allows us to evaluate gradients of quantum circuits without the need for **ancilla qubits** or controlled operations, making it an efficient and practical approach for quantum computing tasks. **Can be extended to multi-qubit hates and gate decompositions to optimise quantum circuits in the NISQ era**

**Error Mitigation**
- Quantum circuits are inherently prone to various types of errors, including decoherence, gate errors, and measurement errors.  These errors can significantly impact the performance and reliability of QNNs. However, the H-QNN model addresses these chal- lenges by incorporating several error mitigation techniques.
	- **Transpilation and Gate Optimisation** (done using Qiskit's transpile function before gate execution)
	- **Measurement Correction** (involves converting the raw counts into probabilities and computing the expectation values of the measured states)

**Model Architecture**
![[Pasted image 20250820134533.png]]
- Input: involves data preprocessing, where images are resized to 720x720 and pixel values normalised
- Classical Neural Network Components: 6 conv layes (Conv2d). Each of these layers are followed by a ReLU activation function, introducing nonlinearity (which assists the model in learning more complex patterns). **Max-pooling**(MaxPool2d) is applied after each convolutional layer. **Pooling reduces the dimensionality of each feature map while retaining the most important information**.  It helps identify consistent features regardless of variations in the size and orientation of objects in the image. Additionally, it also reduces the dimensions of the feature maps which lowers computational load. After final convolutional layer, **an adaptive average pooling layer (AdaptiveAVgPool2d) is applied, which reduces spatial dimensions to a size of (1,1)**
- Fully Connected Layers: The network then transitions from convolutional layers to three fully connected layers (linear). These layers map the learned feature representations to the desired output size. The output of the last convolutional layer sequence is flattened and passed through the fully connected layers to produce a feature vector.
- Quantum Component: these are the hidden layers of H-QNN. The input generated by the last classical layer (i.e., fully connected classical layer) is processed through a quantum circuit, which includes Hadamard gates, parameterized rotation gates (Ry), and measurements. This layer consists of **4 quantum gates** being applied to 2 qubits. To incorporate the quantum circuit into the forward pass of the network, we integrate it with PyTorchâ€™s Function class through an **autograd** function. The final states are then measured to produce a classical output, which is then the input to the final layer of the network.
- Backward pass: During the backward pass, we employ backpropagation to train the network by slightly shifting the inputs using the parameter-shift rule. We compute the gradients of the quantum circuit with respect to the input parameters, enabling the network to learn and update these parameters.  After calculating the gradients, we run the circuit again to verify and adjust the parameters, completing the backward pass.
- **Integration of classical and quantum components**: It is integrated into one of the final layers of the model, specifically after the last fully connected layer of the classical CNN architecture. This final classical layer is responsible for converting the high-level features extracted by the convolutional layers into a format suitable for processing by the quantum circuit. **the measurements from the quantum circuit are the outputs to the hybrid layer**
- Output: Integrates the outcomes of quantum circuit with those from classical to produce the final output

**Validation**
- Used 2000 images for each class in the Car vs.  Bike dataset, 5000 images for each class in the Dump vs. Recycle Waste dataset, and 1500 images for each class in the Football vs. Rugby dataset. Each randomly selected
- Tested performance with limited data by reducing training size to 1000 and 500, and evaluating model performance

**Results**
![[Pasted image 20250820180444.png]]
![[Pasted image 20250820180453.png]]
![[Pasted image 20250820180502.png]]

- When compared to CNN with similar size (six convolutional layers and three fully connected layers), H-QNN outperforms CNN in terms of accuracy (90.01% vs 88.2%)
- H-QNN outperforms VNN across all datasets with 1000 images, and maintains more stable performance when dataset size is reduced to 500, indicating less overfitting than CNN
- H-QNN exhibited well defined, distinct clusters that are well separated. This indicates that H-QNN can learn more useful and seperate features for image classification


**Limitations**
- NISQ era makes implementation difficult due to noise and limited qubits
- Comparison to CNN limited since theyre both the same size, which highlights the large feature space of qubits, but obfuscates actual performance comparisons in terms of computing power
- Mentions several noise mitigation strategies but only discusses two
- Training H-QNN requires significant resources

**Future Work**
- scalability to multi-class classification and larger datasets need to be explored